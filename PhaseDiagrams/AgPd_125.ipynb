{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "from torch import utils\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "use_cuda = torch.cuda.is_available()\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.set_device(3)\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "print (use_cuda)\n",
    "\n",
    "torch.backends.cudnn.enabled\n",
    "import os\n",
    "\n",
    "import sigopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.calculators.emt import EMT\n",
    "from ase.db import connect\n",
    "\n",
    "\n",
    "from clease.tools import update_db\n",
    "from clease import Concentration\n",
    "from clease import CEBulk\n",
    "from clease import Evaluate\n",
    "from clease import NewStructures\n",
    "from clease.calculator import Clease\n",
    "from clease.calculator import attach_calculator\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "with open('agpd_dft_eci_ref_duplication_1_18.json') as json_file: \n",
    "    eci = json.load(json_file) \n",
    "    \n",
    "conc = Concentration(basis_elements=[['Ag', 'Pd']])\n",
    "settings = CEBulk(crystalstructure='fcc',\n",
    "                   a=4.09,\n",
    "                   size=[5,5,5],\n",
    "                   concentration=conc,\n",
    "                   db_name=\"auni_ar.db\",\n",
    "                   max_cluster_size=4,\n",
    "                   max_cluster_dia=[8.0, 6.5, 5.5])\n",
    "\n",
    "atoms = settings.atoms.copy()\n",
    "atoms = attach_calculator(settings, atoms=atoms, eci=eci)\n",
    "\n",
    "gmeans = np.linspace(0,1,51)\n",
    "gmeans_ten = torch.tensor(gmeans).cuda()\n",
    "\n",
    "k_b = 0.00008617\n",
    "\n",
    "def get_concentrations(lattices):\n",
    "    lattices = ((lattices+1)/2).view(lattices.shape[0],-1)\n",
    "    Ag_conc = torch.sum(lattices,dim=1)/(Dim*Dim*Dim)\n",
    "    return Ag_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### Based off Wu et Al. Solving Statistical Mechanics Using Variational Autoregressive Networks\n",
    "\n",
    "\n",
    "def KL_loss(DBG,lattices,epoch,temp,field,num_temps,num_fields):\n",
    "    lattices = lattices.detach()\n",
    "    probs = DBG.get_sample_prob(lattices,temp,field,epoch).view(lattices.shape[0])\n",
    "    with torch.no_grad():\n",
    "        energies = DBG.get_energies(lattices).view(lattices.shape[0])\n",
    "        energies_norm = energies.view(-1)/(temp.view(-1)*k_b)\n",
    "       \n",
    "        F = (energies_norm + probs).view(-1,1)\n",
    "        F_new = F - (field.view(-1,1) * lattices.view(-1,DBG.Nz).sum(dim=1).view(-1,1))/(k_b*temp.view(-1,1))\n",
    "        \n",
    "        batch = int(lattices.shape[0]/(num_temps*num_fields))\n",
    "        F_mean = F_new.view(-1,num_fields).view(num_temps,-1,num_fields)\n",
    "        \n",
    "        F_mean = F_mean.mean(dim=1).view(num_temps,1,num_fields).expand(num_temps,batch,num_fields)\n",
    "        R = (F_new.view(-1) - F_mean.reshape(-1))/torch.abs(F_mean.reshape(-1))\n",
    "        \n",
    "    \n",
    "    assert not R.requires_grad\n",
    "    assert probs.requires_grad\n",
    "    return torch.mean(R*probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_sim = [200,375,550,725,900]\n",
    "field_sim = [-0.2,-0.1,0.0,0.1,0.2]\n",
    "temp_temp = [0]\n",
    "field_temp = [0]\n",
    "coordinate_cutoff = 4.0\n",
    "def train_model(model,optimizer,batch_size,epochs,temp_type,chem_type,temp_range):\n",
    "    field_sim = [-0.2,-0.1,0.0,0.1,0.2]\n",
    "    if temp_range == 'Large':\n",
    "        temps_sim = [100,300,500,700,900]\n",
    "    else:\n",
    "        temps_sim = [200,375,550,725,900]\n",
    "    epoch = 0\n",
    "    temp = torch.zeros(len(temps_sim)*batch_size).cuda()\n",
    "    field = torch.zeros(len(temps_sim)*batch_size).cuda()\n",
    "\n",
    "    print_log_header()\n",
    "\n",
    "    while epoch < epochs:\n",
    "        if temp_type == 'Random Single':\n",
    "            num_temps = 1\n",
    "            t = temps_sim[0] + (temps_sim[-1]-temps_sim[0])*np.random.rand()\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                temp[i] = t\n",
    "        elif temp_type == 'Random':\n",
    "            t_fixed = []\n",
    "            for i in range(5):\n",
    "                t_temp = temps_sim[0] + (temps_sim[-1]-temps_sim[0])*np.random.rand()\n",
    "                t_fixed.append(t_temp)\n",
    "            num_temps = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                temp[i] = t_fixed[int(i/batch_size)]\n",
    "        else:\n",
    "            num_temps = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                temp[i] = temps_sim[int(i/batch_size)]\n",
    "            \n",
    "        if chem_type == 'Random Single':\n",
    "            num_fields = 1\n",
    "            f = -0.2 * 0.4*np.random.rand()\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                field[i] = f\n",
    "        elif chem_type == 'Random':\n",
    "            f_fixed = []\n",
    "            for i in range(5):\n",
    "                f_temp = -0.2 * 0.4*np.random.rand()\n",
    "                f_fixed.append(f_temp)\n",
    "            num_fields = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                field[i] = f_fixed[int(i%(len(field_sim)))]\n",
    "        else:\n",
    "            num_fields = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                field[i] = field_sim[int(i%(len(field_sim)))]\n",
    "        \n",
    "        \n",
    "        epoch = epoch + 1\n",
    "\n",
    "        lattices = model.forward(temp,field)\n",
    "        \n",
    "        \n",
    "        kl_loss = KL_loss(model,lattices,epoch,temp,field,num_temps,num_fields)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        kl_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "       \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log_header():\n",
    "    print ('{:>8} {:>12}'\\\n",
    "       .format('epoch','train loss'))\n",
    "    \n",
    "def print_training_log(epoch, train_loss, test_loss=None):\n",
    "    if test_loss is not None:\n",
    "        print ('{:>8} {:>8} {:>12.4f} {:>12.4f}'\\\n",
    "                   .format(epoch, train_loss, test_loss))\n",
    "        f.write('{:>8} {:>8} {:>12.4f} {:>12.4f}\\n'\\\n",
    "                   .format(epoch, train_loss, test_loss))\n",
    "    else:\n",
    "        print ('{:>8} {:>8}'\\\n",
    "                   .format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_sites(lattices,Nz):\n",
    "    lattices = lattices.view(-1,Nz,2)\n",
    "    lattices = torch.argmax(lattices,dim=2)\n",
    "    lattices = 2*lattices - 1.0\n",
    "    lattices = lattices.view(-1,Nz)\n",
    "    return lattices.float()\n",
    "    \n",
    "def sites_to_one_hot(lattices):\n",
    "    lattices = lattices.view(lattices.shape[0],-1)\n",
    "    lattices = (0.5*(lattices + 1)).long()\n",
    "    one_hot_lattices = torch.zeros(lattices.shape[0],lattices.shape[1],2).float().cuda()\n",
    "    one_hot_lattices = one_hot_lattices.scatter_(2,lattices.view(lattices.shape[0],-1,1),1.0)\n",
    "    return one_hot_lattices.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_weight(m,disp):\n",
    "    num_sites = m.shape[1]\n",
    "    num_components = 2\n",
    "    mask = torch.zeros(m.shape)\n",
    "    for i in range(m.shape[0]):\n",
    "        for j in range(m.shape[1]):\n",
    "            if (disp == 0):\n",
    "                if (j == 0 or j == 1 ):\n",
    "                    mask[i,j] = 1.0\n",
    "                elif ((j-2) < num_components*int(i/num_components) ):\n",
    "                    mask[i,j] = 1.0\n",
    "            if (disp == 1):\n",
    "                if (j == 0 or j==1 ):\n",
    "                    mask[i,j] = 1.0\n",
    "                if ((j-2) < num_components*int(i/num_components) + num_components):\n",
    "                    mask[i,j] = 1.0\n",
    "    mask = mask.detach()\n",
    "    m_masked = m*mask\n",
    "    return m_masked\n",
    "\n",
    "def get_zero_grad_hook(mask):\n",
    "    def hook(grad):\n",
    "        return grad * mask\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoreg_model(nn.Module):\n",
    "    def __init__(self,Nz,assignments):\n",
    "        super().__init__()\n",
    "        self.Nz = Nz\n",
    "        self.Dim = int(math.sqrt(Nz))\n",
    "        self.assignments = assignments\n",
    "        self.activation = activation_functions[assignments['activation']]\n",
    "        self.lsoftmax = torch.nn.LogSoftmax(dim=2)\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "        self.num_layers = int(assignments['Layers'])\n",
    "        \n",
    "        self.shared_layer = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer.weight,0))\n",
    "        self.mask_1 = mask_weight(torch.ones_like(self.shared_layer.weight),0).cuda()\n",
    "        \n",
    "        self.shared_layer.weight.register_hook(get_zero_grad_hook(self.mask_1))\n",
    "        \n",
    "        \n",
    "        self.shared_layer_2 = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer_2.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer_2.weight,1))\n",
    "        self.mask_2 = mask_weight(torch.ones_like(self.shared_layer_2.weight),1).cuda()\n",
    "        self.shared_layer_2.weight.register_hook(get_zero_grad_hook(self.mask_2))\n",
    "        \n",
    "        self.shared_layer_3 = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer_3.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer_3.weight,1))\n",
    "        self.mask_3 = mask_weight(torch.ones_like(self.shared_layer_3.weight),1).cuda()\n",
    "        self.shared_layer_3.weight.register_hook(get_zero_grad_hook(self.mask_3))\n",
    "        \n",
    "        self.shared_layer_4 = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer_4.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer_4.weight,1))\n",
    "        self.mask_4 = mask_weight(torch.ones_like(self.shared_layer_4.weight),1).cuda()\n",
    "        self.shared_layer_4.weight.register_hook(get_zero_grad_hook(self.mask_4))\n",
    "        \n",
    "    def get_sample_prob(self,lattices,temp,field,epoch):\n",
    "        #print(self.shared_layer.weight)\n",
    "        \n",
    "        batch_size = temp.shape[0]\n",
    "        samples = sites_to_one_hot(lattices).view(batch_size,-1)\n",
    "        net_in = torch.cat((field.view(-1,1)*10,samples),dim=1)\n",
    "        net_in = torch.cat((temp.view(-1,1)/1000,net_in),dim=1)\n",
    "        conditional = self.activation(self.shared_layer(net_in))\n",
    "        conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "        conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "        conditional = self.shared_layer_2(conditional)\n",
    "        if self.num_layers > 2:\n",
    "            conditional = self.activation(conditional)\n",
    "            conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "            conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "            conditional = self.shared_layer_3(conditional)\n",
    "            \n",
    "        if self.num_layers > 3:\n",
    "            conditional = self.activation(conditional)\n",
    "            conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "            conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "            conditional = self.shared_layer_4(conditional)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        conditional = conditional.view(-1,self.Nz,2)\n",
    "        Probs = self.lsoftmax(conditional)\n",
    "        samples = samples.detach()\n",
    "        assert Probs.requires_grad == True\n",
    "        site_prob = Probs*(samples.view(-1,self.Nz,2))\n",
    "        probs = torch.sum(torch.sum(site_prob,dim=2),dim=1)\n",
    "        return probs\n",
    "        \n",
    "    def forward(self,temp,field):\n",
    "        \n",
    "        batch_size = temp.shape[0]\n",
    "        temp = temp.view(-1,1)\n",
    "        field = field.view(-1,1)\n",
    "        lattices = torch.zeros(batch_size,2*self.Nz).cuda()\n",
    "        for site in range(self.Nz):\n",
    "            net_in = torch.cat((field*10,lattices),dim=1)\n",
    "            net_in = torch.cat((temp/1000,net_in),dim=1)\n",
    "            conditional = self.activation(self.shared_layer(net_in))\n",
    "            conditional = torch.cat((field*10,conditional),dim=1)\n",
    "            conditional = torch.cat((temp/1000,conditional),dim=1)\n",
    "            conditional = self.shared_layer_2(conditional)\n",
    "            if self.num_layers > 2:\n",
    "                conditional = self.activation(conditional)\n",
    "                conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "                conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "                conditional = self.shared_layer_3(conditional)\n",
    "            \n",
    "            if self.num_layers > 3:\n",
    "                conditional = self.activation(conditional)\n",
    "                conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "                conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "                conditional = self.shared_layer_4(conditional)\n",
    "            \n",
    "            conditional = conditional.view(-1,self.Nz,2)\n",
    "            Probs = self.softmax(conditional)\n",
    "            To_Sample = Probs[:,site,:]\n",
    "            sample = torch.multinomial(To_Sample,1).view(-1,1)\n",
    "            lattices = lattices.view(-1,self.Nz,2)\n",
    "            lattices[:,site,:] = lattices[:,site,:].scatter_(1,sample,1)\n",
    "            lattices = lattices.view(-1,2*self.Nz)\n",
    "        #print(lattices.requires_grad)\n",
    "        lattices = one_hot_to_sites(lattices,self.Nz)\n",
    "        return lattices\n",
    "\n",
    "def create_model(Nz,assignments):\n",
    "    class Discrete_Boltzmann_Generator(nn.Module):\n",
    "        def __init__(self,Nz,assignments):\n",
    "            super().__init__()\n",
    "            self.Nz = Nz\n",
    "            self.Dim = int(math.sqrt(Nz))\n",
    "            self.assignment = assignments\n",
    "            self.Model = autoreg_model(Nz,assignments)\n",
    "            atoms.numbers = np.ones((self.Nz))*46\n",
    "            self.U_pd = atoms.get_potential_energy()\n",
    "            atoms.numbers = np.ones((self.Nz))*47\n",
    "            self.U_ag = atoms.get_potential_energy()\n",
    "            \n",
    "        def forward(self,temp,field):\n",
    "            return self.Model.forward(temp,field)\n",
    "            \n",
    "        def get_energies(self,lattices):\n",
    "            lattices = lattices.view(lattices.shape[0],-1)\n",
    "            energies = torch.zeros(lattices.shape[0]).cuda()\n",
    "            for lattice_num in range(lattices.shape[0]):\n",
    "                energies[lattice_num] = self.get_cluster_energy(lattices[lattice_num,:])\n",
    "            return energies\n",
    "        \n",
    "        def get_cluster_energy(self,lattice):\n",
    "            atoms.numbers = (((lattice + 1)/2)*1 + 46).int().cpu().detach().numpy()\n",
    "            energy_t = atoms.get_potential_energy()\n",
    "            return energy_t\n",
    "        \n",
    "        def get_sample_prob(self,sample,temp,field,epoch):\n",
    "            return self.Model.get_sample_prob(sample,temp,field,epoch)\n",
    "        \n",
    "    model = Discrete_Boltzmann_Generator(Nz,assignments).cuda()\n",
    "    temp_range = assignments['temp_range']\n",
    "    batch_size = int(assignments['batch_size'])\n",
    "    optimizer = optimizers[assignments['optimizer']](model.parameters(), lr=10**assignments['log_learning_rate'])\n",
    "    epochs = assignments['epochs']\n",
    "    temp_type = assignments['temp_type']\n",
    "    chem_type = assignments['Chem_type']\n",
    "    model = train_model(model,optimizer,batch_size,epochs,temp_type,chem_type,temp_range)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'tanh': nn.Tanh(),\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'gradient_descent': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "    'adam': optim.Adam,\n",
    "}\n",
    "\n",
    "\n",
    "Dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ness(model,lattices,tmp,temp_b,field_b):\n",
    "    size = lattices.shape[0]\n",
    "    iters = int(size/1000)\n",
    "    lattices = lattices.view(-1,iters,125)\n",
    "    for i in range(iters):\n",
    "        energies = (model.get_energies(lattices[:,i,:])/(tmp*k_b)).view(-1)\n",
    "        prob = model.get_sample_prob(lattices[:,i,:],temp_b,field_b,1000).view(-1)\n",
    "        temp_pot = -(field_b.view(-1)*lattices[:,i,:].sum(dim=1).view(-1))/(tmp*k_b)\n",
    "        temp_pot = energies+prob+temp_pot\n",
    "        if i ==0:\n",
    "            pot = temp_pot\n",
    "        else:\n",
    "            pot = torch.cat((pot,temp_pot),dim=0)\n",
    "    log_rw = -pot\n",
    "    log_rw = log_rw - log_rw.mean()\n",
    "    log_rw = log_rw.view(-1)\n",
    "    Top_term = 2*torch.logsumexp(log_rw,dim=0)\n",
    "    Bottom_term = torch.logsumexp(2*log_rw,dim=0)\n",
    "    Log_Ness = Top_term - Bottom_term\n",
    "    return torch.exp(Log_Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sigopt import Connection\n",
    "import socket\n",
    "\n",
    "#conn = Connection(client_token=\"RAFA GB Client Token\")\n",
    "experiment_id = 357555\n",
    "assignments = conn.experiments(experiment_id).best_assignments().fetch().data[0].assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignments({\n",
      "  \"Chem_type\": \"Fixed\",\n",
      "  \"Layers\": \"2\",\n",
      "  \"activation\": \"tanh\",\n",
      "  \"batch_size\": \"100\",\n",
      "  \"epochs\": 13285.14463955117,\n",
      "  \"log_learning_rate\": -2.6845144509247336,\n",
      "  \"optimizer\": \"adam\",\n",
      "  \"temp_range\": \"Small\",\n",
      "  \"temp_type\": \"Random\"\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch   train loss\n"
     ]
    }
   ],
   "source": [
    "model = create_model(125,assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"AgPdCE_125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_phase(model,lattices,tmp,temp_b,field_b,Ag_conc):\n",
    "    #print(lattices)\n",
    "    prob = model.get_sample_prob(lattices,temp_b,field_b,1000).view(-1)\n",
    "    with torch.no_grad():\n",
    "        #print('examine_time')\n",
    "        #en_st = time.time()\n",
    "        energies = (model.get_energies(lattices)/(tmp*k_b)).view(-1)\n",
    "        #en_end = time.time()\n",
    "        #print(en_end-en_st)\n",
    "        \n",
    "        pot = -(field_b.view(-1)*lattices.sum(dim=1).view(-1))/(tmp*k_b)\n",
    "        pot = energies+prob+pot\n",
    "        log_rw = -pot\n",
    "        stored = log_rw.max()\n",
    "        log_rw_corr = log_rw - stored\n",
    "        rw = torch.exp(log_rw_corr)\n",
    "        rw_norm = rw/(rw.sum())\n",
    "        est_part = torch.exp(log_rw_corr).mean()\n",
    "        log_z = torch.log(est_part)+stored\n",
    "        #curr_pot = -(k_b*tmp)*(log_z).cpu().detach().numpy().item()\n",
    "        conc_mean = (Ag_conc.view(-1)*rw_norm.view(-1)).sum().cpu().detach().numpy().item()\n",
    "\n",
    "        #n = lattices.shape[0]\n",
    "        var = torch.sum((rw_norm**2)*(Ag_conc-conc_mean)**2)\n",
    "        conc_var = (var).cpu().detach().numpy()\n",
    "        gp = -k_b*tmp*log_z\n",
    "        \n",
    "        energies_ev = (energies*tmp*k_b*rw_norm).sum()\n",
    "        energies_sqrd_ev = (((energies*k_b*tmp)**2)*rw_norm).sum()\n",
    "        \n",
    "        hc = (energies_sqrd_ev - energies_ev**2)/(k_b*tmp**2)\n",
    "        \n",
    "    return conc_mean,conc_var,gp,(energies*(tmp*k_b)*rw_norm).sum(),hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conc_at_temp(temp):\n",
    "    \n",
    "    chemical_pots = np.linspace(-0.2,0.2,41)\n",
    "    concs = np.linspace(-.2,.2,41)\n",
    "    concs_rw = np.linspace(-.2,.2,41)\n",
    "    concs_var = np.linspace(-.2,.2,41)\n",
    "    free_energies = np.linspace(-.2,.2,41)\n",
    "    heat_caps = np.linspace(-.2,.2,41)\n",
    "    temps = torch.zeros(5000).cuda()\n",
    "    counter = 0\n",
    "    for chpot in chemical_pots:\n",
    "        temps = torch.zeros(5000).cuda()\n",
    "        fields = torch.zeros(5000).cuda()\n",
    "        for i in range(5000):\n",
    "            temps[i] = temp\n",
    "            fields[i] = chpot\n",
    "            \n",
    "        lattices = model.forward(temps,fields)\n",
    "        concs_iter = get_concentrations(lattices).view(-1)\n",
    "        concs[counter] = concs_iter.mean().cpu().detach().numpy()\n",
    "        \n",
    "        conc_mean,conc_var,gp,energies,hc = examine_phase(model,lattices,temp,temps,fields,concs_iter)\n",
    "        free_energies[counter] = gp.cpu().detach().numpy()\n",
    "        heat_caps[counter] = hc.cpu().detach().numpy()\n",
    "        concs_var[counter] = conc_var\n",
    "        concs_rw[counter] = conc_mean\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return chemical_pots,concs,concs_rw,concs_var,free_energies,heat_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200. 250. 300. 350. 400. 450. 500. 550. 600. 650. 700. 750. 800. 850.\n",
      " 900.]\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "AgPd_data_concs = np.zeros((15,41))\n",
    "AgPd_data_gp = np.zeros((15,41))\n",
    "AgPd_data_hc = np.zeros((15,41))\n",
    "temps_study = np.linspace(200,900,15)\n",
    "print(temps_study)\n",
    "print()\n",
    "\n",
    "pos = 0\n",
    "for t in temps_study:\n",
    "    chemical_pots,concs,concs_rw,concs_var,gp,hc = get_conc_at_temp(t)\n",
    "    AgPd_data_concs[pos,:] = concs_rw\n",
    "    AgPd_data_gp[pos,:] = gp\n",
    "    AgPd_data_hc[pos,:] = hc\n",
    "    pos+=1\n",
    "    print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"AgPd_AR_125.csv\", AgPd_data_concs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ness(model,lattices,tmp,temp_b,field_b):\n",
    "    size = lattices.shape[0]\n",
    "    iters = int(size/1000)\n",
    "    lattices = lattices.view(-1,iters,125)\n",
    "    for i in range(iters):\n",
    "        energies = (model.get_energies(lattices[:,i,:])/(k_b*tmp)).view(-1)\n",
    "        prob = model.get_sample_prob(lattices[:,i,:],temp_b,field_b,1000).view(-1)\n",
    "        temp_pot = -(field_b.view(-1)*lattices[:,i,:].sum(dim=1).view(-1))/(k_b*tmp)\n",
    "        temp_pot = energies+prob+temp_pot\n",
    "        if i ==0:\n",
    "            pot = temp_pot\n",
    "        else:\n",
    "            pot = torch.cat((pot,temp_pot),dim=0)\n",
    "    log_rw = -pot\n",
    "    log_rw = log_rw - log_rw.mean()\n",
    "    log_rw = log_rw.view(-1)\n",
    "    Top_term = 2*torch.logsumexp(log_rw,dim=0)\n",
    "    Bottom_term = torch.logsumexp(2*log_rw,dim=0)\n",
    "    Log_Ness = Top_term - Bottom_term\n",
    "    return torch.exp(Log_Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "chem_p = np.linspace(-0.2,0.2,41)\n",
    "temps = np.linspace(200,900,21)\n",
    "num = 10000\n",
    "temps_f = torch.zeros(1000).cuda()\n",
    "ch_p_f = torch.zeros(1000).cuda()\n",
    "Ness = np.zeros((temps.shape[0],chem_p.shape[0]))\n",
    "for i in range(temps.shape[0]):\n",
    "    for j in range(chem_p.shape[0]):\n",
    "        for m in range(10):\n",
    "            for k in range(1000):\n",
    "                temps_f[k] = temps[i]\n",
    "                ch_p_f[k] = chem_p[j]\n",
    "            temp_lattices = model.forward(temps_f,ch_p_f)\n",
    "            if m ==0:\n",
    "                lattices = temp_lattices\n",
    "            else:\n",
    "                lattices = torch.cat((lattices,temp_lattices),dim=0)\n",
    "        N_temp = get_Ness(model,lattices,temps[i],temps_f,ch_p_f)\n",
    "        Ness[i,j] = N_temp/num\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ness = np.log10(Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.007049218923878968\n",
      "-3.9997481761097604\n"
     ]
    }
   ],
   "source": [
    "print(np.max(log_ness))\n",
    "print(np.min(log_ness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAFNCAYAAACQSWFgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZxkVX3+8c/TPd2zMDuzwbANi8ggS5QoBkVBFKKIqHEJ7kZRxC3+1IhE474vaIgKasSoURKNERIDKCIoioogyMgiy7A5MDuz90x3fX9/3NtS1NQ9dau7qrum63nPq149fc895566VV2nzrnnfo8iAjMzs27SM94VMDMzG2tu/MzMrOu48TMzs67jxs/MzLqOGz8zM+s6bvzMzKzruPEzM7NxJWlvSd+V9JCkDZL+S9I+bT2m7/MzM7PxImkacAMwAPwjEMCHgGnA4RGxuR3HndSOQs3MzEp6LbA/cHBE3A4g6Ubgj8DrgM+046Du+ZmZ2biRdDkwJSKOqdl+JUBEPKUdx/U1PzMzG0+HAjfV2b4MWNqug7rxMzOz8TQXWFdn+1pgTrsO2vXX/ObN7In95tf/DhBDleKMg0oXPGNKMjnWby9M09x0XiqJegEM7Sgue8dgMmtyFLzBCHls702mq7e43ts3T03m7ZtcfL4AolL8PU69Q8m8laHiekvpJ90zKV320Pa+ZPpI9fYXv8YA9KTfI5F6zg3y0uCtTyWxQ1+6bPUU542h9GuhvvR3+dhefGz1p9+7kagXgAaL/66irz+ZFzU6oQkNLltdd9vA6oiYP/IDPNKJJx0ea1ZvairPb3971zJgW9Wm8yPi/FbVaaS6vvHbb34Pv/rY9LppsXFb3e0AQysnJ8vV8Qcm0wcvur8wrefFhyTz9mzZkE5ft6q4Xg8WpwHEYOKPaSD9h7b9/vSXtN7pWwvT7v3VYcm8Cw+8O5k+uKX4C0P/rPQf68D6GYVpk6akG93+3R9Kpm+8a8/CtNSXgUZm7Lsima6p6XoPrqv/ngfonTaQLrsv/QWqkngtJu2Rfi00rfgjKTalj6v5xc8JoHJf8aTBnn2K3wMAMSX95UyrVhfnXVz8HgCI/sRnSYMvuhpMfwnqO/6O9B9Ok9as3sSvrv1gU3km6aXbIuKoxC7rqN/DK+oRtkTXN35mZlZOEFQq6dGOEVhGdt2v1lLgD60+2DBf8zMzs5KCiMGmHiVcBBwtaf/hDZL2A47J09rCjZ+ZmZUTEDHU1KOELwPLgR9Ieo6kU4AfAPcC57XrqXjY08zMSgmCSrneXPkyIzZLOh74LPANsilVlwNvjYjmZtc0wY2fmZmVFGWHMpsrNeIe4PktLzjBjZ+ZmZXUnsZvPLjxMzOzciKIihs/MzPrNu75mZlZd/Gwp5mZdZ2ASoPQersIN35mZlZKhHt+E0b09hAzZ9ZPW7RHYb5JuxfH8gMY7E8Hp+554aMK0zSYjstYmbJbMl3Ti2OSakuDRZEfKL6tZssdC5NZl/36yGT6ow+7uTBt9ry1ybwR6eC/m9bMLky7+TePTeZ98rMuL0zbsXFaMu/Q5vTrPDUR+3PLquI6A8x53O3FiQ3Cgt7yP09Oph/y3KsK04Y2pAMxDz6Ufv9NPnBjYdqOu9PxNzffP68wbdbjRxemUo9aUJgWPQ2CYk9LP+eYmXgtG8TfHI2hWcXnK3NHi48Y4AkvZmbWVcKNn5mZdSMPe5qZWTcRgdzzMzOzrjKBhj29qoOZmXUd9/zMzKykidPzc+NnZmYlBfKEFzMz6yoBVEotUNvx3PiZmVlJnu1pZmZdJ9zzMzOzLjOBbnVw42dmZqXJPb8JorePypz5dZMq0+oHvAboWflgslhd/rt0+jEHFZe9cX0y7/YlhyXTk8dtENi6snmgMG3K4nQw70XzVyXTJyeCPD9wU/H5ANhjz3TZs/d+oDBtaU8k8/b0FX+TnbrvymTewbXpQM290zcUpg0N9CXz7niw+P03aU5xAHKAR5/ys2T6rT94UmHawc/5eTLv0KapyfTKuuK0SQvT9Z79qEQQ6Gmzknlj+oxkutYXVyxmz0nnbfR3M2v34rwNGovK1HS9U1RpEOG81cLDnmZm1oXc8zMzsy7jnp+ZmXUZRbjnZ2ZmXciNn5mZdRX3/MzMrCtNkMZvzJc0knSMpMskrZS0UdJ1kl5ds88USZ+UtELSVkm/lHRsnbJ6JJ0labmkbZJukPT8sXs2ZmbdJFCl0tSjU41p4yfpcODHQB/wWuB5wG+Ar0o6o2rXr+bp7wVOBlYAl0o6sqbIDwLvA84F/hq4BvhPSc9s49MwM+tOw4Gtm3l0qLEe9nwx0As8OyKG73b9Ud4ovhz4oqQjgNOAV0fE1wAkXQksAz4AnJJvWwC8HfhYRHwqL+sKSQcCHwN+OEbPyczMdjFjPezZD+wAttZsf6iqLqfk+1w4nBgRg8B3gBMlTc43n5iX982asr4JHCZpSWurbmbW7WLC9PzGuvG7IP/5eUl7Spot6bXA04DP5mmHAndFxJaavMvIGrsDq/YbAG6vsx/A0lZW3MzMQFFp6tGpxnTYMyJukvRU4PvAG/LNO4DXR8R38t/nAvWC8K2tSh/+uT4iagM31u5nZmat4NieIyPpIOB7ZL2z15MNfz4H+JKkbRHxrTGqx+nA6QD7LOov3jH1Im+s7ZjWZD3xicn06OktzpsIqN0oL0D0FwceTgXgBehdVBzIefsfd0vmndMg+PTy3z6mMG2vg+9M5t2xOR1M+aH7FhbXa8n9ybxb7q8f2LyMTWtmJ9OVCKodFSXzzp22bUR1AhhcnQ64veQvf1+YFgPpAaFGQbV79plWnFhJBxknNTtweyLoNaANxYHTAWLKlETZ25N5h+bvmUzvXVMcWL0yZ0Eyb2pGZKU/UWegMinx+dUuHTyDsxljPeHlI2Q9vZMjYvidfLmk3YHPSfo2Wa9v3zp5h3tywz27dcBsSarp/dXut5OIOB84H+Bxh+zW4K/RzMyAvOc3MRq/sb7mdxhwQ1XDN+zXwO7AArJe4RJJtV8flwLbefga3zJgMnBAnf0A/tCqSpuZWUaVoaYenWqsG78HgCMl1fbVnwBsI+utXUx2H+ALhhMlTQJeBFwWEcMLzl1C1ot8SU1ZLwVuioi7Wl99M7Nulvf8mnl0qLEe9jwX+E/gYklfILvmdwrwt8BnI2I7cL2kC4FzJPUBdwFnAEuoaugiYqWkzwBnSdoIXEfWQB6fl2lmZq0UdHSD1oyxnu353Tz6yj8AXwGmAHcAZwLnVe36KuDDwIeA2cANwEkRcV1NkWcDm4C3AIuAW4EXRsT/tPN5mJl1p4lzzW/MA1tHxP8B/9dgn63A2/JHar8hsgbyQy2roJmZ1Tcc3mwC8KoOZmZWivLA1hOBGz8zMyvPjZ+ZmXUVT3gxM7Pu4wkvZmbWbYLGIep2EW78zMysPPf8Jobo62fHgvpL//WtLA4Ss+Oxj0+W27NlQzK9UfDqZN6pDRasmLR5xGX37pYOIJ3SMzkdHHjGjI2FaRsemJfM2ygI9NQZxc+5siP9Nl+fOHbvpPS07vmPTgcS2vJg8Ws148B0wO27f/YXhWn7Hlt7y+sj9e2dDj49uKL4de7ZPX2uG37zTwV9n9yXzjsp8Vr1p/NGKi9A/+TitO0DxWlA70Ork+mVGYkA54Ppv4vK9DmFaY2C2Pdsr10a1crq+sbPzMzK8jU/MzPrNr7mZ2ZmXamDV2dvhhs/MzMrKdzzMzOzLuNhTzMz60pu/MzMrJtETJhLfm78zMysCe75mZlZVwlggvT8esa7AmZmtgupNPloE0lvk3SxpBWSQtL7msnvxs/MzMqLJh/t81pgAfDfI8nsYU8zMysnGsfZHUOHRkRF0iTg9c1mduOnHqJ/Wt2kgSVPKMwWPelTN/ne3464SpUp05PpisF0+rbiANKVaYkAvEBlwaLCtKFtDyXzbnpw92T6dbcdXJj2uEffksw7a68Hk+l3/7647MUH3J3MO2Nu8fOqDKUHR2IoHXh4+n4PFKapLx00e68ji89Jz7T0e2DbH9OvRf+84uc8eF86gHRla3+67MMTz2v7jmReBhIBpqckAlMD2pQO6B7zioN5Dy3cO1329m3J9FSg+p5t6XopEZw6ps1K5h1qkN4WHXLNL2J0807d+JmZWXmd0/MbFTd+ZmZWTmcNe46KJ7yYmVlJynp+zTxgnqRrqx6n71SqdEI+Y7PR46eteibu+ZmZWTutjoijGuzzC+CQEmUlVkpujhs/MzMrL1o/7BkRW4D0rLcWc+NnZmblTKBrfg0bP0m9wLOBk4CjgT2BqcBq4FbgSuA/IuKONtbTzMw6QaUzpopIOgrYj4fnriyV9Df5/3+Y9yYLFTZ+kiYDbwXeAiwC7gCuB64GtgJzgSXAWcAHJV0OnB0R14742ZiZWecKddKtDm8EXlH1+wvyB2Rt0/JU5lTP73ZgI/BJ4DsRsaLeTpJ6gOOAlwJXSXpTRHy1VNXNzGyXEm245jcSEfFK4JUjzZ9q/N5ONpyZjM6W32V/OXC5pPcA+460MmZm1uE6ZNhztAobv4i4sGwhkuZHxKqIuA+4ryU1MzOzjhITaMJLYRMu6d1lCpC0B9mkFzMzm9BGdJN7R0oNe35I0paIOKdoB0l7Az8B0lF0O1nvVJhd/97KGCwOEM2OTclity8+In3cweJAuRpKBPcFoi8d+Hpw9+Lg1T1b1zTIu1dh2tQjlifzbltbHNwX4IAFxUGeG32bHNwyJZmeCl49uC0diHm3RcXnZN3yxcm8jQxuqB80HaBvcjrIsyYl4vY2+EyZctDa9A6DxVczKpvT69D0zUsHeWYg8bGyW/H5AIgVGwrTNLnB38Xcucl0BouDgU9akQ5+PjR/z2S6KsXBvIemz0nXKyEmpd+746FTrvmNVqrx+zTwaUnbIuJLtYmSlgBXANOBp7epfmZm1imCrrjm9478dodzJQ1ExNeG0yQdRNbj6weOj4gb219VMzMbbxPlml/yJveIeLOkKcD5eQ/w25KWks3uDOC4iPjDWFTUzMzGm7pi2BOAiDhdUj/wdUn7Am8DBsh6fH9sdwXNzKxDdMOwZ41XkQ1xfhi4m6zhW96uSpmZWWea8MOeki6rs6+ATWTDoNVpEREntr56ZmbWKYLumO05k+y5VvtV/nNGe6pjZmYdKzTxhz0j4uixrIiZmXW+iTLsOTGacDMzsyakwpvNG0mBI81nZmadL0JNPTpVqud3l6SPS9q/USGS+iQ9T9KvgDNaVz0zM+sYw9f8mnl0qNSElxPJ1vJ7u6RfAz8DbgBWkd3nNwfYH3g8WXizfuCzwGfaWWEzMxs/E+WaX2rCyy+AYyQdA/wd8BKyNf4gmwUqYBD4LfBB4OsRsa691W2Dwc1o5W/rJvUsfEJxvr4GwWqjOIguQAxtLUyrNMibCordSPTtlkyvTCs+dmxK12vb5nTQ4uvvK17q8Wnzr0vXa0f6ltT1q4uDGi9Ykl5la9OK+YVpux+8PH3cO4oDgQPM2qc4mLd60gGk6SkObF3ZnD4fPT3p16qyoa847+x0wO3YnP42rzmTixM3b0nnnVZcdkxPB3SvXL8ymd57SPEk9crsBkGxE4GrAbQ9Eai+pzeZNxW8umfLQ8m8QzMXJNNbrVtudQAgIq4GrgaQtA+wJzAFWAPcERHpd7OZmU0ME2g9v7IRXgCIiHuAe9pUFzMz62gionOv4zWjqcbPzMy6XDf2/MzMrLt1zTU/MzMzoHuv+ZmZWfcKX/MzM7Nu1JU9P0kzyW5q3x24NCLWS1JENLhhyczMdnkxca75le6/SvogsAK4DPh3suguAJdJOrsNdTMzsw7TDbE9/0zSO4B3Ap8GnkIW3WXYxcDJra+amZl1mqioqUenKjvs+TrgQxHxQUm1sXr+CBzY2mqZmVmn6cYJL3uThzirYwBIB90zM7NdXxfe6rACeDTwkzppjwHublmNxpj6Z9G39zPrpg0ObS7Op/SpGxrclD5wTyKw8Lo7kllj8qx02dP2KM7bk653pZIIiLzvwmTeP9x+QDJ98YziIL3LbjsombfRtYPHP6F+cHKAjQ+mgxbvNndDYdrA2pnJvDP2WJ1MV29xQOQdq9Kv46Q5GwvTYqD4/QPArHRg657pxcGr1d/om31xwG0AtiYCrw82yNtfHARag+mA272LGpRdSaQPps+XUnmBmFocNDsVuLqR6J864ryWVrb/+j3gvZKOqtoWkpYA/w/4j5bXzMzMOs5EmfBStuf3T8CTgGuA2/Jt3wL2A64HPtLympmZWcfp5AatGaUav4jYJOlJwCvJFrldTbak0eeAr0XE9rbV0MzMOkN09gzOZjRs/CT1AccDN0fEl4Evt71WZmbWcSbSYrYNr/lFxA6ye/l8O4OZWZeL6Gnq0anKXvO7iyykmZmZdbFKt/T8cp8B3i1pTisOKumZkq6StEnSBknXSjq+Kn2OpK9IWi1ps6QfSzqsTjlTJH1S0gpJWyX9UtKxraijmZnViOaiu3Ty9cGyPb+jgPnAckk/I7vvrzqYdUTE68oUJOl1wLn544NkDfCRwLQ8XWTDrPsBbwLWAWcBV0g6MiLuqyruq8CzgHcAdwJnApdKemJE/K7kczMzsxIm0jW/so3fcOzOLcDj6qQHWQi0JEn7AecA74iIc6qSLq36/ynAMcDxEXFFnu+XZEOv7wTenG87AjgNeHVEfC3fdiWwDPhAXo6ZmbVQVzV+EVEcMqQ5ryYLD/GlxD6nAH8abvjy4z8k6WLgOeSNX77fDuDCqv0GJX0HeJekyREx0KJ6m5kZE6fxG+upOE8CbgFeLOkOSYOSbpd0ZtU+hwI31cm7DNhH0vSq/e6KiC119uvHs1PNzForRCV6mnp0qlI9P0kLGu0TEStLFLVn/vgk8G7gDuAFwLmSJkXE54C5wPI6edfmP+cAm/L91iX2KwzoKOl04HSAffbxJFYzszKC7gts/QCPnOBST3FE2of1ADOAV0bEf+XbfpJfCzxL0udL1mdUIuJ84HyAxx21JIqCVE/q3W3kB2lwZocScXQ1/6jiRIBU8GkgKsWBhXsmz0/n7Z1SmDa4OB24ev1AcV6ArYPFwZivXZs+12c89vfJ9J6+4nMyuCMdBHry3OKA20Pb03krO9Iv9Oplxeds0RF/TOYd2jCtMK13Zu2AxyNtumHvZPr0Q+8tTNt6c3pS96TpW5PpvTOL0zU5/TEiigOBs63BVYye9IdyTCp+rbQ9XXZMGXmA6Z5t6SD3lSnFi+JUGgW27inzsdtaE2XYs2zj9wZ2bvx2J5tpuSfwiZLlrAEOAn5Us/0y4CRgD7LeXL2/vuGe3Lqqn/sm9ltbJ83MzEahqxq/iCiaoPKRfIJJw2HR3DLg6ER6Jd/nGXXSlgL3RMTw16hlwHMlTau57rcU2A7cXrJOZmZWRnTfTe4pF5BfPyvh+/nPE2u2nwTcFxEPABcBiyU9ZThR0kzg2XnasIuBPrJrhsP7TQJeBFzmmZ5mZq2VreTeXUsapcwlv0G9hB8CVwDnSZpHdmP6C8h6eq/K97kI+CXwTUnv4OGb3EXV8GpEXC/pQuCcPPj2XcAZwBLgJaN9UmZmNnGVne35+Dqb+8lWcX8vcHWZciIiJJ0KfBR4P9m1vVuAl0TEv+f7VCSdDHwK+AIwhawxPC4iaq/Svwr4MPAhYDZwA3BSRFxXpj5mZtacTu7NNaNsz+8a6s/2FPArsh5XKRGxgSwM2ZmJfdaS3RD/6gZlbQXelj/MzKzNJso1v7KN3zPZufHbBtwdEctbWiMzM+tYXdXzi4hL2l0RMzPrbBETp/ErNdtT0hZJde+8lvQXktJ33JqZ2QQgKtHco1OVHfacQnFDOYls8ouZmU1wE6Xn18ytDjtNeJHUC5yAo6mYmXWFCd/4SXoP8J781wCuztaZfYThwHJfbn3VzMyskwTdMdvzF8CnyW5neCfwbeBPNfsMAH8AvteW2o0BIXoKgsNWKiMvt7fB1dSevuJTP5QITA2g3gYd9kRw6spQg7InzShM27HwkGTeZxx7WTL9cxf/dWHaEXPSl43vW5UOyH3dPfsVpj16/gPJvGuvLg7kvPeSe5J56UkHal6w9M7CtE33paMCzlq6vDCtsjV9pWHKojXJ9Bgq/gCbsm86b2VrOpjyxpv3KUyb+di7k3mZlPjD2b4jnXfWzGSytqXe++m/i0YfBhosrltlRuECMw1pcHsyvdI/iuD7I9EhE14kPYrsdrnjgP2BjcBvgPdExA1lyij8FI2Iy4HL8wMFcG5E3D/aSpuZ2a6qYyaxPIOs4fs6cB1ZkJN3AtdIelJE/LZRAWVvdThrNLU0M7NdX5DF9+wA3wH+JSL+PPQi6Sdka8G+BXh5owJKT3iRNAd4IXAw2ezPahERhRFbzMxsYuiEYc+IWF1n20OSbgMWlymjbGzPA8nia04nu61hAzArT94IbCYRrszMzCaGDhn23ImkuWTxpr9WZv+ySxp9ErgRmE82AeZpwG7AG4H1wNObrqmZme1iOnpJo38ma5/OKbNz2cbv8cC5wPCUvJ6I2BoRXwC+VPZgZma264p8MdsmI7zMk3Rt1WOn9V8lnSApSjx+Wq9eks4CTgPeGBGlFjIve81vFrA6X25oA7B7VdqvgHeXLMfMzHZhI+jNrY6IuuExq/wCSN9LldnpnihJrwc+AvxjRPxr2UqVbfzuBoZvSroNeC5waf77M8iuAZqZmTUtIraQre3aFEkvI1v39dMR8eFm8pZt/H5MFsbse8DngG9IeiIwCBxJ1QrrZmY2cVU641YHJD2XbHLLVyLi7c3mL9v4nQVMA4iIb0naDrwo3/b3ZNcDzcxsAgs641YHSceSRR27AbhA0tFVyQMRcX2jMho2fnnw6r2BlcPbIuI/gf9susZmZrYL65gIL8cDk4HHAlfXpN0N7NeogDKzPQO4iWzGp5mZdbFOuNUhIt4XESp47FemjIY9v3yG5/3sHNVlwisKeF1Go6DYUvGpzzrb7RE96ZcxFVR7cPLsZN7pp6af9M3fLP6u9es1k5N5D5+9MJn+d0ddV5g2Z+GqZN6+6VsL07asKg56DTBl5uZk+uY/FQfknvmodNDswfXFQYt3PDQ9mXfb+nT6rEfdm0xP6Z2XDgI988ji4NWxtcGHYU/xe0iJNIDKnevSRS9KvPcHh5J51ZPuJ1SmFb9W2tEgmPxgIhD9lPTrGL3pv5tWC2AU8f47Stn7/L4CvEnt/FQ2M7POFp3R82uFshNeBDwa+KOkHwIreOTithERH2115czMrLN0yDW/USvb+L2/6v9vqJMegBs/M7MJrkNWdRi1so3f1LbWwszMOl50zmzPUSu7nt9AuytiZmadrxKN99kVlF7PD0DSM4BjyWJ7fiQi7s1vLrwrIh5sRwXNzKxzdNWwp6SZwMXAk4EBsjX9vgzcC7yZ7Ab4t7apjmZm1gGCiTPhpeytDp8AHkW2jt9MeETT/6N8u5mZTWSRLWvUzKNTlR32fC7wroi4os69fncD+7S2WmZm1ok6JbD1aJVt/GaSDXHWMxnwze9mZhNcpwS2boWyw55/JAskWs+TgWWtqY6ZmXWu5lZx7+Trg2V7fucBn5a0Bvj3fNs0SX8LvCl/mJmZ7RLK3uf3L5IOIpv4Mrxw7ZVkveDPRcQF7anerqtRUOxKpTiQbirodbv1JgJfD/WmYx0MHPDYZPqGHYOFaesSAbUBVmxNB/i9cfn+hWl7r5+VzLto4crCtJmLVifzbntoRjJ9xuLisiub00HGB9YXl71jc/q1mL5X8XGBdADp3nTo4qGV6WPHUOK93yA4dc9g8S3FsbU/mXfHxmnJ9KnzNhYnThtd3H4N7ihMiwZBsZX4LKAn/VmgKP6bapcOnsPSlNKfshHxVklfAE4E5gNrgMsi4uZ2Vc7MzDrHRLrVoakuRkTcBtzWprqYmVmHmyhLGpVu/CQJeDHwRGAxcD/wC+DCiE6+m8PMzFplosz2LBvhZS/gh8BjgNXAg2SzPN8InCXpmRFxf9tqaWZm4y5i4gx7lr3V4VxgEXBCRCyIiMMiYgHw9Hz7ue2qoJmZdY5o8tGpyg57ngC8OSJ+Ur0xIi6X9G7gcy2vmZmZdZyJ0vMr2/htBf5UkPanPN3MzCawYOJMeCk77Plt4DUFaa/h4RvfzcxswhIRzT06Vdme3w3AqZKuBb5LNuFlIfACYB7wv5JOG945ItwYmplNQBOl51e28fty/nMvoF4Yj69U/T9wT9DMbMKZSIGtyzZ+h7S1FmZmtkuodPIUziaUje15a7srYmZmnW+CtH3NhTez1kkFvk4FvR6t0QTN7p+8KJk+MDsROBi4dMu/FKYt3O2oZN4z9zggmb5tsPh8Llu5RzLv/gffXpg2uC0dTHny9C3J9AdvXlKYNmfPVcm8/XM2FKZtWjU3Xa8t6UDNW1bMK0ybtnBtMu+flh2YTF944N2FaUPb+5J5+2ckzmeDoNh9c9Lvvx23F7+WfftuSuaNzeljpwYCY27xuQbo2by+uNzp6de5MjWd3moT6Sb3shFeJgNnkU1w2Ruo/cuKiJjc4rqZmVmH6bYJL/8MvAq4DLgE2N62GpmZWcfqtgkvzwPeFRGfbmdlzMysc3XjTe6DwI3trIiZmdlYKdv4fQN4fjsrYmZmnS+iuUenKjvseRbwZUkXAZcC62p3cFQXM7OJr5Kc27rrKNv4HU62ssNi4OQ66Y7qYmY2wQVddpM7cB6wBXgRcAue7Wlm1pU6eSizGWUbv0OBF0bE/7SzMmZm1snUdcOefwR8E7uZWTfr8EkszSg72/Ns4GxJe7azMmZm1rmG7/Nr5tGpyvb83kK2ft/tkpax82zPiIgTW1ozMzPrON024WUGcE/+GP7dJqBU4OuIdMDt3qnpANKrzygeOHjaBfsk8167Jv2WS/1BvmLpbcm8m9bOLkx7YOX8ZN6bV6WDfR80tzh49YKDlyfzrrlt38K0qbPSgZhvvvbwZPqGrdMK0w5feksyb1/fjmT6LYlj77HHg8m80xNpQwPpoNiTd6Q/zu747WMK0/bfcHMy79SDVibT2V58TnoeWJHMWtkr/d7vNBOk7SirPQ4AABslSURBVCu9pNHR7a6ImZl1tuxWh+6a8GJmZtZ1E16QtFDSRyT9XNLNkpbm298gKb0Ym5mZTQhdNeFF0qOBq4A+4DfAE3l4Tb+Dgb8CXtqOCpqZWWcIuq/n9yngLmAJ8EweuXDx1WSNoZmZTXATpedXtvF7CvCRiFjPzpN9HgDS0/wSJF0iKSR9qGb7HElfkbRa0mZJP5Z0WJ38UyR9UtIKSVsl/VLSsSOtj5mZFYhsZnUzj05V+pofUDTPfXdg60gOLulvgSPqbBdwMXAS8Cay5ZT6gCsk7VWz+1eB1wLvJQu6vQK4VNKRI6mTmZnVFyN4dKqyjd+1wMsK0p4PXNPsgSXNAT4LvK1O8inAMcDLIuLbEXFJvq0HeGdVGUcApwF/HxFfjojLgReS3Y/4gWbrZGZm3aFs4/dh4PmSLgZeQNagHyvpPLLG5iMjOPbHgZsi4tt10k4B/hQRVwxviIiHyHqDz6nZbwdwYdV+g8B3gBMlOR6pmVkLddWwZ0T8mKyRO4Js3T4BnwGeRbbaw9XNHFTSk4CXA2cW7HIocFOd7cuAfSRNr9rvrojYUme/fuDAZuplZmZp3baSOxHxX5K+T9bgLADWAL+PiKYm9EjqJ1sf8FMRcWvBbnOB5XW2r81/zgE25fvttKp81X5zm6mbmZkVGw5sPREUNn6S7gSeGxE3DG+LiKB+j6wZ7wSmkg2ljgtJpwOnA+yzz+7jVQ0zs11OJw9lNiPV89uPFq/hJ2kfsuWRXgNMrrkmN1nSbGAjWW9uTp0ihnty66p+1ov+O7zf2jppRMT5wPkARx21/7i8lJVKOkh0u2SXRNsjFRQbYNI/Prk47wXp8/HVdZck00+ZelJh2h2rFyTzptIPXpgOSrz/7DXJ9L33LM5/ZyLQMkBPT/F37EaBrQeHepPpC2atL0y7+daDknl3m7wtmX7RHQcUpr35gOXJvINbphQn9qT/VAfWpYOfz579UGHa9o3Fgb4B+tcm6gVM6k+ck6n9ybzRV1x29KRfx/EwQdq+pm51aIX9ySLDfJOs4Rp+ALw9//9hZNfsDq2TfylwT0QM/+UvA5ZIqn3nLgW2A7e3tPZmZl0sC2zdHRNeWl313wHH1XlA1iAeR9ZgXQQslvSU4YySZgLPztOGXUx2/98LqvabBLwIuCwiBlpcfzOz7tXkZJddecLL+yWtLlFORMQrSuy0Hvhp7fbsnnbujoif5r9fBPwS+Kakd5D1CM8im2X6iaryrpd0IXCOpD6yEGxnkIVhe0mJepuZWRM6YcKLpBlkAU4eSxZhbAdwG/D5iPhmmTIaNX5HAmV6Ty1t3yOiIulkspiiXyAbKv0lcFxE3Fuz+6vIJs98CJgN3ACcFBHXtbJOZmbdbnjYswP0A4PAR8nuDJhMNuL3DUnzI+KzjQpo1PidGhG/Hm0tG4nYeXXEiFgLvDp/pPJuJYsSUy9SjJmZtVAntH0RsYYsule1H0p6FFmbMerGz8zM7M86pOdXZA0PL7eX5MbPzMxK66RJLPkiCL3ALLI40ycCf1cmrxs/MzMrpQMjvJwJ/HP+/x3AWyLi38pkLGz8ImKs7wE0M7MOV2m+6zdP0rVVv5+fBxr5M0knAD8qUdaVEfHUqt8vJFtVaB7ZQgf/LGkoIs5rVJB7fmZmVtoIRj1XR8RRDfb5BXBIibIesYhBRKwCVuW/XpIHPPmUpH+NiB2pgtz4mZlZKdGmqC35yjy3tKCoa4FXAAuB+1I7emjTzMwmiqeQrfizstGO7vlNQKMJXh1RHGC6Mtqg2NP3Kky6V5clsx7Vc1wyfcOO4nrft3m3ZN5D5tRbFSvzp3XpVT92SwU0Btatm12YdsuDeybzrtpWPGP76IF0zPl7Hkqv5rV/76rCtOlTtibzfuHGdODrExcXB5Beef+iZN5UQO5Fix9I5n37Ren3yJmPubMwbfUd6aU/H7/9hmT6nBnFYYR7dkv3MXpW3V+cODf9HolJpWb1t1AQHXCnn6TXAUcDPybr4e1Otubs3wDviojtjcpw42dmZqV0UISX3wPPIYsCNhdYDdwMnBwR/1umADd+ZmZWWifc6hARvwCeOZoy3PiZmVlp0Ul3uY+CGz8zMyulA29yHzE3fmZmVpp7fmZm1nXc8zMzs66SzfZ0z8/MzLpMJ9zn1wpu/MzMrDQPe5qZWVcJgop7fmZm1lXC1/zMzKwL+ZqfdZ1GAbMrQ+kgzyn3/29xcGmA6SdclEyfO+WAwrQbH5yazPu2nuK8R8xLB4ffMZT+E9q2vb8wrRJK5j168T2Faas2zkrmHUgEiAaIxLGvvm/fZN4XLEmfk9TzuvLOdFDsvaZvLEy75p4lybybB9NXo9ZvLQ5w/rhDlyXzblqfPt89NxbXbc4xy5N5Y9EeyfROkt3k7sbPzMy6jBs/MzPrMp2xpFEreDFbMzPrOu75mZlZKb7mZ2Zm3UdQ0cS4zd2Nn5mZleaen5mZdZXIY7xMBG78zMysNPf8zMys6/ian5mZdZVs0NONn5mZdRk3fjYqPT3FsRcrlaExrEkLNYj9GYnYnwMHPSGZd/vgt5PpG3cUx218+axnJvP+9/0DhWmTe+Yl8x4wKx2T9LbVCwvTZvYXHxfgoS3Fz+lTf5idzPvsPdNxQ+9cW/y8rnqwL5n3tg0LkulTEmFFf76uOHYnwPP2KH5e03rTH7prdmxPpn/+D7sXpv3bX6RfCyl9naunt/hvduD2Gcm8famQpT3pj2hV0n9zrecJL2Zm1mUCX/MzM7Ou42t+ZmbWhYJd9LJMDTd+ZmZWimd7mplZV3LjZ2ZmXSY87GlmZt0lW9JoYvT8vJitmZl1Hff8zMysNN/kbmZmXSao+JqfmZl1k8A9PzMz6zpBJdzzsw4lFb+s0SD4dFslgvBqMB1YuJHtQ8UBk89b9ZVk3qC4XuvufUEy79Hr9kimHz67+Hmt2jY5mfeqlcURop++MB1oeeNgOrD1vywvDsg9KdLBp0/ZPR1Ue3uleB7d5sF0kOeexGsxuz8duHrx5OnJ9CPmFpd91a//Mpl3ye4r08fe577CtMpAfzIvlURPKhEAH0ADD6XLbgP3/MzMrMv4Pj8zM+syAVTCPT8zM+sqXs/PzMy6TUB4wouZmXUTr+pgZmZdKXzNz8zMuotne5qZWRdyz8/MzLqMZ3uamVmXCTzb08zMuk542NPMzLqPhz3NaiUCagPQkwi4nUgDGPjV4cn0pU/bszDt8J79knl/V7mzMG3qUDr4dCUdX5p7thQHNf76+uuTeR9TeUxh2o8eTB/3N5Urk+l9PVML014840nJvNcXx8QG4PMPfKkw7aDdnpXMu2Zgr8K0Kb3pwNV7TEvX6ysr7y9Me+nQ3sm8Iv1Cb9te/Do/9tFXpCvWP7c4rZIeYuxfeVe6bCvkxs/MzMoJz/Y0M7Ou49meZmbWZTzb08zMulCAe35mZtZtfM3PzMy6zMS55tczlgeT9DeSvifpbklbJd0q6aOSZtTsN0fSVyStlrRZ0o8lHVanvCmSPilpRV7eLyUdO3bPyMys21SafLSfpBdLCkn3lc0zpo0f8HZgCHg3cBLwReAM4EeSegAkCbg4T38T8HygD7hCUu1NQF8FXgu8FzgZWAFcKunI9j8VM7MuFJXmHm0maTZwDvBAM/nGetjz2RGxqur3KyWtBb4OPBX4CXAKcAxwfERcASDpl8BdwDuBN+fbjgBOA14dEV/Lt10JLAM+kJdjZmYt05HDnp8AbiDr/JxQNtOY9vxqGr5hv8l/Ls5/ngL8abjhy/M9RNYbfE5VvlOAHcCFVfsNAt8BTpSUDs1hZmYj0DnDnpKOAV4KnNls3rEe9qznKfnPm/OfhwI31dlvGbCPpOlV+90VEVvq7NcPHNjqipqZdb2I5h5tIqkPOB/4ZETc3mz+cW38JC0mG6L8cURcm2+eC9SLHrg2/zmn5H6JgHlmZta8aPpfG/0DMBn46Egyj9utDnkP7gfAIPCqMT726cDp+a8Dk/TSej1NS5sHrB7vSjzsusKUO0ZRaqOwwddsbbrI0uftAX7edOGt8IUtxedytG7b9P10enFSW99v79vUYIe7R1H4zxrt8PsRppVy8GgLqHEpDM5rMs8USddW/X5+RJxfvYOkE4AflSjryoh4qqQDgbOB50bEtibrA4xT4ydpKtk1vP2Bp0RE9fTUdTzcu6s2typ9+Oe+if3W1kkDID/x5+d1uTYijipfewOft5HyeRsZn7eRqWl0Ri0iTmpleVV+ARxSYr/hy1yfJ5sgeU0+2xOyy13Kfx+IiORX0zFv/PJx2u8CRwFPj4jarzbLgGfUyboUuCciNlXt91xJ02qu+y0FtgNNjwGbmdnYyz/Db2kiy1Kyzk+9S1/rgM8Bb00VMNY3ufcA3wKOB06NiGvq7HYRsFjSU6ryzQSenacNu5js/r8XVO03CXgRcFlEDLT+GZiZWQd4MXBczeNSsqHx44BzGxUw1j2/fyFrrD4MbJZ0dFXaffnw50XAL4FvSnoHWSt+FiCy+zkAiIjrJV0InJP3Ju8iu2F+CfCSJup0fuNdrA6ft5HxeRsZn7eRmZDnrV7HSdIryYY7f1qmDEUbp6LudDBpOfWv0wG8PyLel+83F/gUcCowhawxfFtE3FBT3lSyhvQ0YDbZjY7/UPbJm5nZxCDpAuCEiKiNBFZ//7Fs/MzMzDpBJ9zkPmYk9Ug6S9JySdsk3SDp+SMoZ39JW/JAqhP+ZvqRnjdJMyW9V9IvJK2RtD7//6ljUe+xImlvSd+V9JCkDZL+S9I+JfN2bXD2kZ43SUdJOl/SLfnf4T2SviVpyVjUe7yN5v1WU8678s+w8bmnZpx1VeMHfBB4H9nF0L8GrgH+U9IzmyznC8BDra1aRxvpedsHeANwJVkIoheR3cr1fUlNhyPqRJKmkU25fjTwCuBlwEFkgdh3K1FEVwZnH+V5ezFZhKfPk70f3wU8FrhW0t5tq3QHaMH7bbic/YF/BFa2o567hIjoigewABggu7ZYvf1y4MYmyjkNeJBsGm0AB473c+vU8wbsBkyrs/1ysttWxv35teD8vIVspZIDq7YtIQve8LYGeY/I30Ovqto2CbgVuGi8n1sHn7f5dbbtSxZI8gPj/dw69bzVlHMpcB7wU+Dn4/28xuPRTT2/E8lugvxmzfZvAoeVGTKRNAf4DNnSTOtbXsPONOLzFhGbY+fYqwDXAnu2rorj6hTgmqiKLRgRdwFX88hA7EV5uzU4+4jPW9QJkB8RdwOreDhA/kQ1mvcbAJJOI+spn9WWGu4iuqnxO5SsB1N78/uy/OfSEmV8ArglIr7Ryop1uFact1rH0twNrZ0sFYi90bnp5uDsozlvO5F0CNkoxc2N9t3Fjeq85V/gPwu8MyIKo2B1g3GL7TkO5gLrI+/zVykVCFvSk4GXA3/Rhrp1slGdt1p5XNWjya4BTgSpAOv1wvSVzTucPlGN5rw9Qh7c4ktkPb+vjr5qHW205+2TZNfdL2hhnXZJu2zPT9IJ+UylRo+ftuBY/WTj45+NiD+MuvLjaCzPW51jP5VsksK/RcS3Wl2+da1zgb8CXhoR9RoG4xFf4M+o82W26+zKPb9mA6GuA2ZLUs0L3zAQNtnkljnA56uCqE7Lf86QNCMiNpas93gby/P2Z5L+kix6z0+A15Ss664gFYi90QfxiIOzTwCjOW9/JuljZCu0vCIiLmtR3TrZaM7beWQ94/uqPscmAb3571uji8JC7rKNXzQfCHUZ2dpPB/DI61fD4+SpHt1SYBFwf52068giy+wSU9PH+LwBIOkwstllvwOeHxE7mjh+p1tGdh2m1lIan5tuDs4+mvMGgKSzydZ0e1MXXYcfzXk7JH+8vk7aOuDvgXNGVbtdyC477DkCl5DNrKuN+/lS4KZ8xlSRj7FzENWPV+WfSD2ZWqM5b0g6iGydrjuBk6PBMiO7oIuAo/P7pgCQtB9wDI8MxF5PNwdnH815Q9KbgQ8BZ0dEwyDGE8hozlvtZ9hxZF/cb8r//93WV7eDjfe9FmP5IGvEtgFvA54KfJHs3qCTa/a7HLi9QVmvpAvu8xvNeSObfbecbPjuWWQTXaofk8f7ubXg3OxG1kP7PdlU81PIPlDuBKZX7bcv2b1Y763J/x2yb92vAZ5G9gG0DXjseD+3Tj1vZDe5V4D/q/OeWjrez61Tz1tBeT+lS+/z22WHPUfobGAT2Y2ii8huJn5hRPxPzX697MJDwm0w0vM2vOYWQO2+kN2cu7ylNR1jEbFZ0vFk08e/Qbb6yOXAW+PhtSfJt/ey82jLq8iCs3+Ih4OznxQR7VtOvQOM8rydlG8/KX9Uu5LsC9qE1IL3m+Uc2NrMzLqOvxWYmVnXceNnZmZdx42fmZl1HTd+ZmbWddz4mZlZ13HjZ2ZmXceNn5mZdR03fl1O0hMl/YekP0naLmmNpB9JeoWk3nyfV+YrPXTE+nKSLpC0vE1lv09Sw5tfJf20ZhWMFZIukfSEERzzVElvG1mNmzrOfvnz279O2nJJF4ygzFLnK993t/x99jdNlN8naZWkHyb2eVr+Grwy//2/JX2h7DGsO7nx62KS3kq2AvRcsgDBJwCvJlvv64vAyeNXu6QPAs8d70oANwJPzB9/D+wFXJkvrNqMU8lCx7XbfsA/ATs1fmTn84NtPv7/A1YD3yubIbIg6P8OPEPSwoLdXg5s5uHYlO8HXivpUaOoq01wbvy6lKRjgc8A50bECRHxjYi4KiJ+EBFnAocByaDV4yUi7oiI68e7HsDGiLgmf3yH7MvCZOCMca5X0yLi+oi4o13lS5oMvAk4L5oPK/V1slBdp9UpdzfgecD3h8N75e+N68mWIjOry41f9/oHsoDT76yXmDcwN9ZsnifpW5I25MNXn5c0pXoHSdMkfVzSXfkw6l2SzpbUU7XPU/NhqlMlnSdpraT1ks6R1CvpLyX9XNJmScsknVhzjJ2GPfMhtY9JukPSgKQHJH1vuLcgaX5+rNskbZF0r6R/l7R45Kdwp3O2nGw18T8PD0s6WNL38+e3VdI1kk6qSr8AeAWwuGoIdXlV+nxJX5J0f/68bpF0es1zHx6WPrro9VG2kPAVeZYfVR3rqXn6I4Y923C+TiUbYbiwNkHSUyRdLmlj/ppfKukxw+l5nNObgJfVKfd5wHSyBrLad4CXSJo6wvraBOfGrwvl1/KOI1s2Z1sTWb8B3EH2gfNF4EzgrKpyJ5Gt2/ca4HPAXwNfAd4DfLJOeeeQDVe9CPhnssDZ5wD/Bvxrfpy1wH9Jmpd4Pv1kyya9CbiArAf2xjzv8MKfc8lWSziLLBjyO4CDgKtrG/CRkjQrP876/Pc9gZ8DR+T1eWGe9r+S/jrP9kHgh2SN5vAQ6nPz/DPz/M8E3ke2MsbFwBclvalOFVKvz3X57wBvrjpWUQDtVp+vk4CbI2J19UZJzyILzLyJbJms04AZwM8k7V2169eBv5BUu5bdy4D7yBZJrnYVMJPsOZrtbLyXlfBj7B/AQrLlmD5acv9X5vu/v2b7/wC3Vf3+sny/Y2v2O5tscdYF+e9Pzff715r9rsu3P6lq2+H5tldUbbsAWF71+6vzfU5p4hz0Anvn+Z5btf192Z9Fw/w/JWuYJuWPA4D/zss7Nd/nU2TLyhxYc9xbgetqns99dY7xHrIG6KCa7V8mu3Y2qcnXZ/i8n1DnWMuBC9p4vm4GvlVn++3A5TXbZubP75yqbXvk5/LjVdv2BIbqvY/J1kkcAt49Fn9Tfux6D/f8rBn/W/P774F9qn4/Cbgb+IWkScMP4DKyD6Oja/L/X83vtwCbI+LnNdsg++At8gzggYhILuYp6QxJN0jaRPZBek+edHAqX8IxZAv97iD7EP8r4PUR8d95+rHANRHx5xXZI2II+DZwZN6zSzkJ+BVwV835vBTYnWzJqGqNXp+mtPh87UnWu60u/yCyLw3fqnl+W4Bfkp0/ACJiBdn76CVVQ+gvJRu9qh3yJLKJMg/lxzXbides605rgK08vNZeWWtrfh8gm+AxbEFe5o6C/LvX/L6u5vft5EOGwyJiuySA1FDb7sD9iXTyYcLPk03yeUd+7B7gmgZlp9xANsQbwIPA/RFRPZljLtnEi1oPkK23NgfYkCh/Adn1w7Lns9HrU1obzteUvD7VFuQ/v5o/at1T8/vXya7lHQ/8mGyk4dcRcUttxtxWwNf8rC43fl0oIgYl/RR4uqTJEVH7oTRSa8hmiL6wIH15i45TazXwmAb7vJhseO3/DW+QtGSUx90UEdcm0teSLf5baxFZg1nb+NdaA6wkuxZaz60NazhyrT5fa3j4+mv1NsiuK/64Tp7tNb//gKw39zJJa8he8zcmjjmX7L1hthM3ft3rY2TXrT5BnQ/X/INuRuw84zPlEuD5ZI1C0bfxdrgMeLGkZ0fExQX7TGPnXtar2lstrgTeKmm/yGaCDk82ehFwfUQM12eA+j2US8gm8dwTEStbUJ/hLzllekOtPl+3sPP9hbeSfSE6NCI+1qiAiNgm6UKySTHbyBrHb9fbV9Iist5mO78g2C7MjV+XioirlEUV+YykpWSTLu4h+3b+NLLhvNPIbuQu61tkH5CXS/o02bBgP9l1nVPIJoJsadmTeNg3gdcC35b0UbLrZDOAE8kmTdxC1pD8g6R3A78mGzorHWlkhD5LNhnlR5L+iawxeQPwKLKZm8P+AMyVdAZwLbAtIn6f538R2czHz5J9kO8GPBp4ckQ8p8n63EZ27e7VktaSNYa3RsTGOvu2+nxdRfZFoCciKpDNkpF0JvCDfMbuf5D11BaSXT+9JyI+U1PO14HTyV7v70dE7VDvsOFIO1eNos42gbnx62IRcY6kX5NFJ/kUMA/YSPYB/DqyafXNlLcjvyfvXWQfUEvIbmW4g2wyRu0wVkvkx30GWfSS0/Ofa8ii1wx/OH4AmE32XKeQ9cpOBO5sR53yev1J0pOAj5PdejAZ+B3wrIi4pGrXr5BNBvpIXse7gf0i4iFJfwW8l+y+zMVk10RvpYkoKVX1WSPpjXlZV5LN4DyObASgVqvP14Vkr8uT87KG6/RDZQEXziY7D1PJroleQ517AiPiF5L+SHbbxb8ljncy8NvqyUZm1fTI6/NmZu2RX2e+PSJe0+bjTAFWAG+PiHoTacx8q4OZjZmzyW5VaFlUnQKvI5sotNMtEGbD3PiZ2ZiIiKvJhlGbvcWmWQPAKyNisM3HsV2Yhz3NzKzruOdnZmZdx42fmZl1HTd+ZmbWddz4mZlZ13HjZ2ZmXef/A9cZ8tl4CqYVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_chem_p = []\n",
    "all_temps = []\n",
    "for i in range(temps.shape[0]):\n",
    "    for j in range(chem_p.shape[0]):\n",
    "        all_temps.append(temps[i])\n",
    "        all_chem_p.append(2*chem_p[j])\n",
    "        \n",
    "\n",
    "plt.hist2d(all_chem_p,all_temps,bins = (41,21), weights = np.log10(Ness).reshape(-1),cmap='inferno',vmin=-4.0,vmax=0.0)\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "plt.xlabel('Chemical Potential (eV)',fontsize=16,**hfont)\n",
    "plt.ylabel('Temperature (K)',fontsize=16,**hfont)\n",
    "plt.xticks(fontsize = 16,fontname = \"Arial\") \n",
    "plt.yticks(fontsize = 16,fontname = \"Arial\") \n",
    "plt.yticks([200,400,600,800])\n",
    "plt.xticks([-0.4,-0.2,0.0,0.2,0.4])\n",
    "\n",
    "cb = plt.colorbar()\n",
    "\n",
    "imaxes = plt.gca()\n",
    "plt.axes(cb.ax)\n",
    "plt.yticks(fontsize=16,fontname = \"Arial\")\n",
    "cb.set_ticks([-4.0,-3.0,-2.0,-1.0,0.0])\n",
    "\n",
    "\n",
    "plt.savefig(\"NESS_AgPd_128.pdf\",bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:james_mlmat] *",
   "language": "python",
   "name": "conda-env-james_mlmat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
