{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/james_mlmat/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "from torch import utils\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "use_cuda = torch.cuda.is_available()\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.set_device(0)\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "print (use_cuda)\n",
    "\n",
    "torch.backends.cudnn.enabled\n",
    "import os\n",
    "\n",
    "import sigopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.calculators.emt import EMT\n",
    "from ase.db import connect\n",
    "\n",
    "\n",
    "from clease.tools import update_db\n",
    "from clease import Concentration\n",
    "from clease import CEBulk\n",
    "from clease import Evaluate\n",
    "from clease import NewStructures\n",
    "from clease.calculator import Clease\n",
    "from clease.calculator import attach_calculator\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "with open('agpd_dft_eci_final_submission.json') as json_file: \n",
    "    eci = json.load(json_file) \n",
    "    \n",
    "conc = Concentration(basis_elements=[['Ag', 'Pd']])\n",
    "settings = CEBulk(crystalstructure='fcc',\n",
    "                   a=4.09,\n",
    "                   size=[5,5,5],\n",
    "                   concentration=conc,\n",
    "                   db_name=\"agpd_dft_ref_final_submission.db\",\n",
    "                   max_cluster_size=4,\n",
    "                   max_cluster_dia=[8.0, 6.5, 5.5])\n",
    "\n",
    "atoms = settings.atoms.copy()\n",
    "atoms = attach_calculator(settings, atoms=atoms, eci=eci)\n",
    "\n",
    "gmeans = np.linspace(0,1,51)\n",
    "gmeans_ten = torch.tensor(gmeans).cuda()\n",
    "\n",
    "k_b = 0.00008617\n",
    "\n",
    "def get_concentrations(lattices):\n",
    "    lattices = ((lattices+1)/2).view(lattices.shape[0],-1)\n",
    "    Ag_conc = torch.sum(lattices,dim=1)/(Dim*Dim*Dim)\n",
    "    return Ag_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### Based off Wu et Al. Solving Statistical Mechanics Using Variational Autoregressive Networks\n",
    "\n",
    "\n",
    "def KL_loss(DBG,lattices,epoch,temp,field,num_temps,num_fields):\n",
    "    lattices = lattices.detach()\n",
    "    probs = DBG.get_sample_prob(lattices,temp,field,epoch).view(lattices.shape[0])\n",
    "    with torch.no_grad():\n",
    "        energies = DBG.get_energies(lattices).view(lattices.shape[0])\n",
    "        energies_norm = energies.view(-1)/(temp.view(-1)*k_b)\n",
    "       \n",
    "        F = (energies_norm + probs).view(-1,1)\n",
    "        F_new = F - (field.view(-1,1) * lattices.view(-1,DBG.Nz).sum(dim=1).view(-1,1))/(k_b*temp.view(-1,1))\n",
    "        \n",
    "        batch = int(lattices.shape[0]/(num_temps*num_fields))\n",
    "        F_mean = F_new.view(-1,num_fields).view(num_temps,-1,num_fields)\n",
    "        \n",
    "        F_mean = F_mean.mean(dim=1).view(num_temps,1,num_fields).expand(num_temps,batch,num_fields)\n",
    "        R = (F_new.view(-1) - F_mean.reshape(-1))/torch.abs(F_mean.reshape(-1))\n",
    "        \n",
    "    \n",
    "    assert not R.requires_grad\n",
    "    assert probs.requires_grad\n",
    "    return torch.mean(R*probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_sim = [200,375,550,725,900]\n",
    "field_sim = [-0.2,-0.1,0.0,0.1,0.2]\n",
    "temp_temp = [0]\n",
    "field_temp = [0]\n",
    "coordinate_cutoff = 4.0\n",
    "def train_model(model,optimizer,batch_size,epochs,temp_type,chem_type,temp_range):\n",
    "    field_sim = [-0.2,-0.1,0.0,0.1,0.2]\n",
    "    if temp_range == 'Large':\n",
    "        temps_sim = [100,300,500,700,900]\n",
    "    else:\n",
    "        temps_sim = [200,375,550,725,900]\n",
    "    epoch = 0\n",
    "    temp = torch.zeros(len(temps_sim)*batch_size).cuda()\n",
    "    field = torch.zeros(len(temps_sim)*batch_size).cuda()\n",
    "\n",
    "    print_log_header()\n",
    "\n",
    "    while epoch < epochs:\n",
    "        if temp_type == 'Random Single':\n",
    "            num_temps = 1\n",
    "            t = temps_sim[0] + (temps_sim[-1]-temps_sim[0])*np.random.rand()\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                temp[i] = t\n",
    "        elif temp_type == 'Random':\n",
    "            t_fixed = []\n",
    "            for i in range(5):\n",
    "                t_temp = temps_sim[0] + (temps_sim[-1]-temps_sim[0])*np.random.rand()\n",
    "                t_fixed.append(t_temp)\n",
    "            num_temps = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                temp[i] = t_fixed[int(i/batch_size)]\n",
    "        else:\n",
    "            num_temps = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                temp[i] = temps_sim[int(i/batch_size)]\n",
    "            \n",
    "        if chem_type == 'Random Single':\n",
    "            num_fields = 1\n",
    "            f = -0.2 * 0.4*np.random.rand()\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                field[i] = f\n",
    "        elif chem_type == 'Random':\n",
    "            f_fixed = []\n",
    "            for i in range(5):\n",
    "                f_temp = -0.2 * 0.4*np.random.rand()\n",
    "                f_fixed.append(f_temp)\n",
    "            num_fields = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                field[i] = f_fixed[int(i%(len(field_sim)))]\n",
    "        else:\n",
    "            num_fields = 5\n",
    "            for i in range(len(temps_sim)*batch_size):\n",
    "                field[i] = field_sim[int(i%(len(field_sim)))]\n",
    "        \n",
    "        \n",
    "        epoch = epoch + 1\n",
    "\n",
    "        lattices = model.forward(temp,field)\n",
    "        \n",
    "        \n",
    "        kl_loss = KL_loss(model,lattices,epoch,temp,field,num_temps,num_fields)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        kl_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "       \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log_header():\n",
    "    print ('{:>8} {:>12}'\\\n",
    "       .format('epoch','train loss'))\n",
    "    \n",
    "def print_training_log(epoch, train_loss, test_loss=None):\n",
    "    if test_loss is not None:\n",
    "        print ('{:>8} {:>8} {:>12.4f} {:>12.4f}'\\\n",
    "                   .format(epoch, train_loss, test_loss))\n",
    "        f.write('{:>8} {:>8} {:>12.4f} {:>12.4f}\\n'\\\n",
    "                   .format(epoch, train_loss, test_loss))\n",
    "    else:\n",
    "        print ('{:>8} {:>8}'\\\n",
    "                   .format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_sites(lattices,Nz):\n",
    "    lattices = lattices.view(-1,Nz,2)\n",
    "    lattices = torch.argmax(lattices,dim=2)\n",
    "    lattices = 2*lattices - 1.0\n",
    "    lattices = lattices.view(-1,Nz)\n",
    "    return lattices.float()\n",
    "    \n",
    "def sites_to_one_hot(lattices):\n",
    "    lattices = lattices.view(lattices.shape[0],-1)\n",
    "    lattices = (0.5*(lattices + 1)).long()\n",
    "    one_hot_lattices = torch.zeros(lattices.shape[0],lattices.shape[1],2).float().cuda()\n",
    "    one_hot_lattices = one_hot_lattices.scatter_(2,lattices.view(lattices.shape[0],-1,1),1.0)\n",
    "    return one_hot_lattices.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_weight(m,disp):\n",
    "    num_sites = m.shape[1]\n",
    "    num_components = 2\n",
    "    mask = torch.zeros(m.shape)\n",
    "    for i in range(m.shape[0]):\n",
    "        for j in range(m.shape[1]):\n",
    "            if (disp == 0):\n",
    "                if (j == 0 or j == 1 ):\n",
    "                    mask[i,j] = 1.0\n",
    "                elif ((j-2) < num_components*int(i/num_components) ):\n",
    "                    mask[i,j] = 1.0\n",
    "            if (disp == 1):\n",
    "                if (j == 0 or j==1 ):\n",
    "                    mask[i,j] = 1.0\n",
    "                if ((j-2) < num_components*int(i/num_components) + num_components):\n",
    "                    mask[i,j] = 1.0\n",
    "    mask = mask.detach()\n",
    "    m_masked = m*mask\n",
    "    return m_masked\n",
    "\n",
    "def get_zero_grad_hook(mask):\n",
    "    def hook(grad):\n",
    "        return grad * mask\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoreg_model(nn.Module):\n",
    "    def __init__(self,Nz,assignments):\n",
    "        super().__init__()\n",
    "        self.Nz = Nz\n",
    "        self.Dim = int(math.sqrt(Nz))\n",
    "        self.assignments = assignments\n",
    "        self.activation = activation_functions[assignments['activation']]\n",
    "        self.lsoftmax = torch.nn.LogSoftmax(dim=2)\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "        self.num_layers = int(assignments['Layers'])\n",
    "        \n",
    "        self.shared_layer = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer.weight,0))\n",
    "        self.mask_1 = mask_weight(torch.ones_like(self.shared_layer.weight),0).cuda()\n",
    "        \n",
    "        self.shared_layer.weight.register_hook(get_zero_grad_hook(self.mask_1))\n",
    "        \n",
    "        \n",
    "        self.shared_layer_2 = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer_2.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer_2.weight,1))\n",
    "        self.mask_2 = mask_weight(torch.ones_like(self.shared_layer_2.weight),1).cuda()\n",
    "        self.shared_layer_2.weight.register_hook(get_zero_grad_hook(self.mask_2))\n",
    "        \n",
    "        self.shared_layer_3 = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer_3.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer_3.weight,1))\n",
    "        self.mask_3 = mask_weight(torch.ones_like(self.shared_layer_3.weight),1).cuda()\n",
    "        self.shared_layer_3.weight.register_hook(get_zero_grad_hook(self.mask_3))\n",
    "        \n",
    "        self.shared_layer_4 = nn.Linear(2*self.Nz+2,2*self.Nz,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.shared_layer_4.weight.copy_(np.sqrt(2)*mask_weight(self.shared_layer_4.weight,1))\n",
    "        self.mask_4 = mask_weight(torch.ones_like(self.shared_layer_4.weight),1).cuda()\n",
    "        self.shared_layer_4.weight.register_hook(get_zero_grad_hook(self.mask_4))\n",
    "        \n",
    "    def get_sample_prob(self,lattices,temp,field,epoch):\n",
    "        #print(self.shared_layer.weight)\n",
    "        \n",
    "        batch_size = temp.shape[0]\n",
    "        samples = sites_to_one_hot(lattices).view(batch_size,-1)\n",
    "        net_in = torch.cat((field.view(-1,1)*10,samples),dim=1)\n",
    "        net_in = torch.cat((temp.view(-1,1)/1000,net_in),dim=1)\n",
    "        conditional = self.activation(self.shared_layer(net_in))\n",
    "        conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "        conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "        conditional = self.shared_layer_2(conditional)\n",
    "        if self.num_layers > 2:\n",
    "            conditional = self.activation(conditional)\n",
    "            conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "            conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "            conditional = self.shared_layer_3(conditional)\n",
    "            \n",
    "        if self.num_layers > 3:\n",
    "            conditional = self.activation(conditional)\n",
    "            conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "            conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "            conditional = self.shared_layer_4(conditional)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        conditional = conditional.view(-1,self.Nz,2)\n",
    "        Probs = self.lsoftmax(conditional)\n",
    "        samples = samples.detach()\n",
    "        assert Probs.requires_grad == True\n",
    "        site_prob = Probs*(samples.view(-1,self.Nz,2))\n",
    "        probs = torch.sum(torch.sum(site_prob,dim=2),dim=1)\n",
    "        return probs\n",
    "        \n",
    "    def forward(self,temp,field):\n",
    "        \n",
    "        batch_size = temp.shape[0]\n",
    "        temp = temp.view(-1,1)\n",
    "        field = field.view(-1,1)\n",
    "        lattices = torch.zeros(batch_size,2*self.Nz).cuda()\n",
    "        for site in range(self.Nz):\n",
    "            net_in = torch.cat((field*10,lattices),dim=1)\n",
    "            net_in = torch.cat((temp/1000,net_in),dim=1)\n",
    "            conditional = self.activation(self.shared_layer(net_in))\n",
    "            conditional = torch.cat((field*10,conditional),dim=1)\n",
    "            conditional = torch.cat((temp/1000,conditional),dim=1)\n",
    "            conditional = self.shared_layer_2(conditional)\n",
    "            if self.num_layers > 2:\n",
    "                conditional = self.activation(conditional)\n",
    "                conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "                conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "                conditional = self.shared_layer_3(conditional)\n",
    "            \n",
    "            if self.num_layers > 3:\n",
    "                conditional = self.activation(conditional)\n",
    "                conditional = torch.cat((field.view(-1,1)*10,conditional),dim=1)\n",
    "                conditional = torch.cat((temp.view(-1,1)/1000,conditional),dim=1)\n",
    "                conditional = self.shared_layer_4(conditional)\n",
    "            \n",
    "            conditional = conditional.view(-1,self.Nz,2)\n",
    "            Probs = self.softmax(conditional)\n",
    "            To_Sample = Probs[:,site,:]\n",
    "            sample = torch.multinomial(To_Sample,1).view(-1,1)\n",
    "            lattices = lattices.view(-1,self.Nz,2)\n",
    "            lattices[:,site,:] = lattices[:,site,:].scatter_(1,sample,1)\n",
    "            lattices = lattices.view(-1,2*self.Nz)\n",
    "        #print(lattices.requires_grad)\n",
    "        lattices = one_hot_to_sites(lattices,self.Nz)\n",
    "        return lattices\n",
    "\n",
    "def create_model(Nz,assignments):\n",
    "    class Discrete_Boltzmann_Generator(nn.Module):\n",
    "        def __init__(self,Nz,assignments):\n",
    "            super().__init__()\n",
    "            self.Nz = Nz\n",
    "            self.Dim = int(math.sqrt(Nz))\n",
    "            self.assignment = assignments\n",
    "            self.Model = autoreg_model(Nz,assignments)\n",
    "            atoms.numbers = np.ones((self.Nz))*46\n",
    "            self.U_pd = atoms.get_potential_energy()\n",
    "            atoms.numbers = np.ones((self.Nz))*47\n",
    "            self.U_ag = atoms.get_potential_energy()\n",
    "            \n",
    "        def forward(self,temp,field):\n",
    "            return self.Model.forward(temp,field)\n",
    "            \n",
    "        def get_energies(self,lattices):\n",
    "            lattices = lattices.view(lattices.shape[0],-1)\n",
    "            energies = torch.zeros(lattices.shape[0]).cuda()\n",
    "            for lattice_num in range(lattices.shape[0]):\n",
    "                energies[lattice_num] = self.get_cluster_energy(lattices[lattice_num,:])\n",
    "            return energies\n",
    "        \n",
    "        def get_cluster_energy(self,lattice):\n",
    "            atoms.numbers = (((lattice + 1)/2)*1 + 46).int().cpu().detach().numpy()\n",
    "            energy_t = atoms.get_potential_energy()\n",
    "            return energy_t\n",
    "        \n",
    "        def get_sample_prob(self,sample,temp,field,epoch):\n",
    "            return self.Model.get_sample_prob(sample,temp,field,epoch)\n",
    "        \n",
    "    model = Discrete_Boltzmann_Generator(Nz,assignments).cuda()\n",
    "    temp_range = assignments['temp_range']\n",
    "    batch_size = int(assignments['batch_size'])\n",
    "    optimizer = optimizers[assignments['optimizer']](model.parameters(), lr=10**assignments['log_learning_rate'])\n",
    "    epochs = assignments['epochs']\n",
    "    temp_type = assignments['temp_type']\n",
    "    chem_type = assignments['Chem_type']\n",
    "    model = train_model(model,optimizer,batch_size,epochs,temp_type,chem_type,temp_range)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'tanh': nn.Tanh(),\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'gradient_descent': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "    'adam': optim.Adam,\n",
    "}\n",
    "\n",
    "\n",
    "Dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ness(model,lattices,tmp,temp_b,field_b):\n",
    "    size = lattices.shape[0]\n",
    "    iters = int(size/1000)\n",
    "    lattices = lattices.view(-1,iters,125)\n",
    "    for i in range(iters):\n",
    "        energies = (model.get_energies(lattices[:,i,:])/(tmp*k_b)).view(-1)\n",
    "        prob = model.get_sample_prob(lattices[:,i,:],temp_b,field_b,1000).view(-1)\n",
    "        temp_pot = -(field_b.view(-1)*lattices[:,i,:].sum(dim=1).view(-1))/(tmp*k_b)\n",
    "        temp_pot = energies+prob+temp_pot\n",
    "        if i ==0:\n",
    "            pot = temp_pot\n",
    "        else:\n",
    "            pot = torch.cat((pot,temp_pot),dim=0)\n",
    "    log_rw = -pot\n",
    "    log_rw = log_rw - log_rw.mean()\n",
    "    log_rw = log_rw.view(-1)\n",
    "    Top_term = 2*torch.logsumexp(log_rw,dim=0)\n",
    "    Bottom_term = torch.logsumexp(2*log_rw,dim=0)\n",
    "    Log_Ness = Top_term - Bottom_term\n",
    "    return torch.exp(Log_Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-6943380969a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#conn = Connection(client_token=\"RAFA GB Client Token\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexperiment_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m357555\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0massignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_assignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "from sigopt import Connection\n",
    "import socket\n",
    "\n",
    "#conn = Connection(client_token=\"RAFA GB Client Token\")\n",
    "experiment_id = 357555\n",
    "assignments = conn.experiments(experiment_id).best_assignments().fetch().data[0].assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = {\n",
    "  \"Chem_type\": \"Fixed\",\n",
    "  \"Layers\": \"2\",\n",
    "  \"activation\": \"tanh\",\n",
    "  \"batch_size\": \"100\",\n",
    "  \"epochs\": 13285.14463955117,\n",
    "  \"log_learning_rate\": -2.6845144509247336,\n",
    "  \"optimizer\": \"adam\",\n",
    "  \"temp_range\": \"Small\",\n",
    "  \"temp_type\": \"Random\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch   train loss\n"
     ]
    }
   ],
   "source": [
    "model = create_model(125,assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"AgPdCE_125_final_submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_phase(model,lattices,tmp,temp_b,field_b,Ag_conc):\n",
    "    #print(lattices)\n",
    "    prob = model.get_sample_prob(lattices,temp_b,field_b,1000).view(-1)\n",
    "    with torch.no_grad():\n",
    "        #print('examine_time')\n",
    "        #en_st = time.time()\n",
    "        energies = (model.get_energies(lattices)/(tmp*k_b)).view(-1)\n",
    "        #en_end = time.time()\n",
    "        #print(en_end-en_st)\n",
    "        \n",
    "        pot = -(field_b.view(-1)*lattices.sum(dim=1).view(-1))/(tmp*k_b)\n",
    "        pot = energies+prob+pot\n",
    "        log_rw = -pot\n",
    "        \n",
    "        log_rw_ns = log_rw - log_rw.mean()\n",
    "        log_rw_ns = log_rw_ns.view(-1)\n",
    "        Top_term = 2*torch.logsumexp(log_rw_ns,dim=0)\n",
    "        Bottom_term = torch.logsumexp(2*log_rw_ns,dim=0)\n",
    "        Log_Ness = Top_term - Bottom_term\n",
    "    \n",
    "        stored = log_rw.max()\n",
    "        log_rw_corr = log_rw - stored\n",
    "        rw = torch.exp(log_rw_corr)\n",
    "        rw_norm = rw/(rw.sum())\n",
    "        est_part = torch.exp(log_rw_corr).mean()\n",
    "        log_z = torch.log(est_part)+stored\n",
    "        #curr_pot = -(k_b*tmp)*(log_z).cpu().detach().numpy().item()\n",
    "        conc_mean = (Ag_conc.view(-1)*rw_norm.view(-1)).sum().cpu().detach().numpy().item()\n",
    "\n",
    "        gp = -k_b*tmp*log_z\n",
    "        \n",
    "    return conc_mean,gp,Log_Ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conc_at_temp(temp):\n",
    "    \n",
    "    chemical_pots = np.linspace(-0.2,0.2,81)\n",
    "    concs = np.linspace(-.2,.2,81)\n",
    "    concs_rw = np.linspace(-.2,.2,81)\n",
    "    gps = np.linspace(-.2,.2,81)\n",
    "    num_samps = np.linspace(-.2,.2,81)\n",
    "    temps = torch.zeros(5000).cuda()\n",
    "    counter = 0\n",
    "    for chpot in chemical_pots:\n",
    "        temps = torch.zeros(5000).cuda()\n",
    "        fields = torch.zeros(5000).cuda()\n",
    "        for i in range(5000):\n",
    "            temps[i] = temp\n",
    "            fields[i] = chpot\n",
    "            \n",
    "        lattices = model.forward(temps,fields)\n",
    "        concs_iter = get_concentrations(lattices).view(-1)\n",
    "        \n",
    "        conc_mean,gp,num_samp = examine_phase(model,lattices,temp,temps,fields,concs_iter)\n",
    "        gps[counter] = gp.cpu().detach().numpy()\n",
    "        concs_rw[counter] = conc_mean\n",
    "        num_samps[counter] = num_samp\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return chemical_pots,concs_rw,gps,num_samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200. 250. 300. 350. 400. 450. 500. 550. 600. 650. 700. 750. 800. 850.\n",
      " 900.]\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "AgPd_data_concs = np.zeros((15,81))\n",
    "AgPd_data_gp = np.zeros((15,81))\n",
    "AgPd_data_num_samp = np.zeros((15,81))\n",
    "temps_study = np.linspace(200,900,15)\n",
    "print(temps_study)\n",
    "print()\n",
    "\n",
    "pos = 0\n",
    "for t in temps_study:\n",
    "    chemical_pots,concs_rw,gp,num_samp = get_conc_at_temp(t)\n",
    "    AgPd_data_concs[pos,:] = concs_rw\n",
    "    AgPd_data_gp[pos,:] = gp\n",
    "    AgPd_data_num_samp[pos,:] = num_samp\n",
    "    pos+=1\n",
    "    print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"AgPd_AR_125_concs_81_final_submission_3.csv\", AgPd_data_concs, delimiter=\",\")\n",
    "np.savetxt(\"AgPd_AR_125_samps_81_final_submission_3.csv\", AgPd_data_num_samp, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ness(model,lattices,tmp,temp_b,field_b):\n",
    "    size = lattices.shape[0]\n",
    "    iters = int(size/1000)\n",
    "    lattices = lattices.view(-1,iters,125)\n",
    "    for i in range(iters):\n",
    "        energies = (model.get_energies(lattices[:,i,:])/(k_b*tmp)).view(-1)\n",
    "        prob = model.get_sample_prob(lattices[:,i,:],temp_b,field_b,1000).view(-1)\n",
    "        temp_pot = -(field_b.view(-1)*lattices[:,i,:].sum(dim=1).view(-1))/(k_b*tmp)\n",
    "        temp_pot = energies+prob+temp_pot\n",
    "        if i ==0:\n",
    "            pot = temp_pot\n",
    "        else:\n",
    "            pot = torch.cat((pot,temp_pot),dim=0)\n",
    "    log_rw = -pot\n",
    "    log_rw = log_rw - log_rw.mean()\n",
    "    log_rw = log_rw.view(-1)\n",
    "    Top_term = 2*torch.logsumexp(log_rw,dim=0)\n",
    "    Bottom_term = torch.logsumexp(2*log_rw,dim=0)\n",
    "    Log_Ness = Top_term - Bottom_term\n",
    "    return torch.exp(Log_Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_p = np.linspace(-0.2,0.2,41)\n",
    "temps = np.linspace(200,900,21)\n",
    "num = 10000\n",
    "temps_f = torch.zeros(1000).cuda()\n",
    "ch_p_f = torch.zeros(1000).cuda()\n",
    "Ness = np.zeros((temps.shape[0],chem_p.shape[0]))\n",
    "for i in range(temps.shape[0]):\n",
    "    for j in range(chem_p.shape[0]):\n",
    "        for m in range(10):\n",
    "            for k in range(1000):\n",
    "                temps_f[k] = temps[i]\n",
    "                ch_p_f[k] = chem_p[j]\n",
    "            temp_lattices = model.forward(temps_f,ch_p_f)\n",
    "            if m ==0:\n",
    "                lattices = temp_lattices\n",
    "            else:\n",
    "                lattices = torch.cat((lattices,temp_lattices),dim=0)\n",
    "        N_temp = get_Ness(model,lattices,temps[i],temps_f,ch_p_f)\n",
    "        Ness[i,j] = N_temp/num\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"AgPd_125_NESS_data_final_submission\", Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ness = np.log10(Ness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(log_ness))\n",
    "print(np.min(log_ness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chem_p = []\n",
    "all_temps = []\n",
    "for i in range(temps.shape[0]):\n",
    "    for j in range(chem_p.shape[0]):\n",
    "        all_temps.append(temps[i])\n",
    "        all_chem_p.append(2*chem_p[j])\n",
    "        \n",
    "plt.hist2d(all_chem_p,all_temps,bins = (41,21), weights = np.log10(Ness).reshape(-1),cmap='inferno',vmin=-4.0,vmax=0.0)\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "plt.xlabel('Chemical Potential (eV)',fontsize=16)\n",
    "plt.ylabel('Temperature (K)',fontsize=16)\n",
    "plt.xticks(fontsize = 16,fontname = \"Arial\") \n",
    "plt.yticks(fontsize = 16,fontname = \"Arial\") \n",
    "plt.yticks([200,400,600,800])\n",
    "plt.xticks([-0.4,-0.2,0.0,0.2,0.4])\n",
    "\n",
    "cb = plt.colorbar()\n",
    "\n",
    "imaxes = plt.gca()\n",
    "plt.axes(cb.ax)\n",
    "plt.yticks(fontsize=16,fontname = \"Arial\")\n",
    "cb.set_ticks([-4.0,-3.0,-2.0,-1.0,0.0])\n",
    "\n",
    "\n",
    "plt.savefig(\"NESS_AgPd_125_Final_Submission.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:james_mlmat] *",
   "language": "python",
   "name": "conda-env-james_mlmat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
